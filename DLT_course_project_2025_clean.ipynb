{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning in Human Language Technology Project\n",
    "\n",
    "- Student(s) name(s): Nouman Bashir\n",
    "- Date: 07/11/2025\n",
    "- Chosen Corpus: Rotten Tomatoes (sentence-level sentiment)\n",
    "- Contributions (if group project): None\n",
    "\n",
    "### Corpus information\n",
    "\n",
    "- Description of the chosen corpus: The dataset contains 2 features as text and label, label 1 represents positive whereas 0 represents negative. The training dataset contains 8530 rows of data, while the validation and test datasets contain 1066 each rows.\n",
    "- Paper(s) and other published materials related to the corpus: Bo Pang and Lillian Lee. 2005. \"Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales.\" Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL'05), pages 115-124. ArXiv: https://arxiv.org/abs/cs/0506075\n",
    "This project uses the Rotten Tomatoes movie review dataset (Pang and Lee, 2005), which contains 10,662 sentences labeled with binary sentiment (positive/negative). The dataset was originally introduced for sentiment classification research and has become a standard benchmark in the NLP community.\n",
    "- Random baseline performance and expected performance for recent machine learned models: ~50% is the random baseline, and the expected perofrmance on pre-trained BERT is 87-89% and SOTA is ~92%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Setup Installation and Importations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/84.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing device: cuda\n",
      "GPU: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "\n",
    "!pip install -q transformers datasets torch accelerate evaluate scikit-learn\n",
    "\n",
    "# Import libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Check and display device information\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Data download, sampling and preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Download the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e86376a0683f462092a299c0687fdd4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e30f9c692304adfabbc364788e6ccb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.parquet:   0%|          | 0.00/699k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1f5e4e49806480480db118c645f0dc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation.parquet:   0%|          | 0.00/90.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e662e6e043554e67b380ef35e9c23e2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.parquet:   0%|          | 0.00/92.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6067bfd1ae7948a6b9d0eb3af3a66ed8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/8530 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8681673a6961413199610c17235b05fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f2976987f4245fe9fcf75d1e8d438f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 8530\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 1066\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 1066\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Your code to download the corpus here\n",
    "\n",
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(dataset['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples in train: 8530\n",
      "Number of examples in validation: 1066\n",
      "Number of examples in test: 1066\n"
     ]
    }
   ],
   "source": [
    "for split in dataset:\n",
    "    print(f\"Number of examples in {split}: {len(dataset[split])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Sampling and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to split into negative and positive count\n",
    "for split in dataset:\n",
    "    labels=[ex['label'] for ex in dataset[split]]\n",
    "    neg_count = labels.count(0)\n",
    "    pos_count = labels.count(1)\n",
    "    total = len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST SET:\n",
      "Total examples: 1066\n",
      "Negative (0): 533 (50.0%)\n",
      "Positive (1): 533 (50.0%)\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{split.upper()} SET:\")\n",
    "print(f\"Total examples: {total}\")\n",
    "print(f\"Negative (0): {neg_count} ({neg_count/total*100:.1f}%)\\nPositive (1): {pos_count} ({pos_count/total*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Baseline on Test Set: 0.4719\n"
     ]
    }
   ],
   "source": [
    "# Calculate random baseline\n",
    "\n",
    "def calculate_random_baseline(dataset_split):\n",
    "    labels = [ex['label'] for ex in dataset_split]\n",
    "    random_predictions = [random.randint(0, 1) for _ in labels]\n",
    "    return accuracy_score(labels, random_predictions)\n",
    "\n",
    "random_baseline = calculate_random_baseline(dataset['test'])\n",
    "print(f\"\\nRandom Baseline on Test Set: {random_baseline:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Prompting a generative model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Prompt optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: HuggingFaceTB/SmolLM-135M-Instruct\n",
      "Description: 135M parameter instruction-tuned generative model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba242d3c20f94818aa4942aa065ff70b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4fe4eb69b5d48c4a9bb47251a6d4b5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7d1f9c389c145098c6c1a2943ea78cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76f5da1ad84b4839a9ca9aeebe61fa58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4a1121843bc460484a48229d3b4fa72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/565 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09286238a59e4099a36a6885c6617484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/723 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6cdfa3f11f24d4580e42fcbcf4d062e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/269M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aff776f853e41dcacf63fb98a04aaaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/156 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing prompt templates:\n",
      "\n",
      "  Testing 'Simple' prompt...\n",
      "Accuracy: 0.8400\n",
      "\n",
      "  Testing 'Few-shot' prompt...\n",
      "Accuracy: 0.9400\n",
      "\n",
      "Best prompt template: 'Few-shot'\n",
      "Validation accuracy: 0.9400\n"
     ]
    }
   ],
   "source": [
    "# Your code and experiments relating to the prompt optimization here\n",
    "\n",
    "gen_model_name = \"HuggingFaceTB/SmolLM-135M-Instruct\"\n",
    "print(f\"\\nModel: {gen_model_name}\")\n",
    "print(\"Description: 135M parameter instruction-tuned generative model\")\n",
    "\n",
    "# Loading and tokenizing model\n",
    "gen_tokenizer = AutoTokenizer.from_pretrained(gen_model_name)\n",
    "gen_model = AutoModelForCausalLM.from_pretrained(gen_model_name).to(device)\n",
    "\n",
    "if gen_tokenizer.pad_token is None:\n",
    "    gen_tokenizer.pad_token = gen_tokenizer.eos_token\n",
    "\n",
    "prompt_templates = {\n",
    "    \"Simple\": \"\"\"Classify sentiment: {text}\n",
    "Answer (positive/negative):\"\"\",\n",
    "\n",
    "    \"Few-shot\": \"\"\"Classify movie review sentiments.\n",
    "\n",
    "Review: a masterpiece of form and execution\n",
    "Sentiment: positive\n",
    "\n",
    "Review: simplistic , silly and tedious.\n",
    "Sentiment: negative\n",
    "\n",
    "Review: {text}\n",
    "Sentiment:\"\"\"\n",
    "}\n",
    "\n",
    "def evaluate_prompt(template, model, tokenizer, val_samples, max_samples=100):\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    invalid = 0\n",
    "    samples = val_samples.select(range(min(max_samples, len(val_samples))))\n",
    "    for i, example in enumerate(samples):\n",
    "        prompt = template.format(text=example['text'])\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=5,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.pad_token_id\n",
    "            )\n",
    "        generated = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True).strip().lower()\n",
    "        if 'positive' in generated:\n",
    "            pred = 1\n",
    "        elif 'negative' in generated:\n",
    "            pred = 0\n",
    "        else:\n",
    "            pred = random.randint(0, 1)\n",
    "            invalid += 1\n",
    "        predictions.append(pred)\n",
    "        labels.append(example['label'])\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    return accuracy, invalid\n",
    "\n",
    "print(\"\\nTesting prompt templates:\")\n",
    "prompt_results = []\n",
    "\n",
    "for name, template in prompt_templates.items():\n",
    "    print(f\"\\n  Testing '{name}' prompt...\")\n",
    "    acc, invalid = evaluate_prompt(template, gen_model, gen_tokenizer, dataset['validation'])\n",
    "    prompt_results.append({\n",
    "        'Template': name,\n",
    "        'Accuracy': acc,\n",
    "    })\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "\n",
    "# Select best prompt\n",
    "prompt_df = pd.DataFrame(prompt_results).sort_values('Accuracy', ascending=False)\n",
    "best_prompt_name = prompt_df.iloc[0]['Template']\n",
    "best_prompt_template = prompt_templates[best_prompt_name]\n",
    "\n",
    "print(f\"\\nBest prompt template: '{best_prompt_name}'\")\n",
    "print(f\"Validation accuracy: {prompt_df.iloc[0]['Accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating SmolLM-135M with prompting on 1066 examples.\n",
      "\n",
      "Test Set Results:\n",
      "Accuracy:  0.6782\n",
      "Precision: 0.6227\n",
      "Recall:    0.9043\n",
      "F1 Score:  0.7376\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model_prompting(model, tokenizer, dataset_split, template, model_name=\"Model\"):\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    invalid_count = 0\n",
    "    print(f\"\\nEvaluating {model_name} with prompting on {len(dataset_split)} examples.\")\n",
    "    for i, example in enumerate(dataset_split):\n",
    "        if i % 100 == 0:\n",
    "            print(f\"  Progress: {i}/{len(dataset_split)}\", end='\\r')\n",
    "        prompt = template.format(text=example['text']) #\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=5,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.pad_token_id\n",
    "            )\n",
    "        generated = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True).strip().lower()\n",
    "        if 'positive' in generated:\n",
    "            pred = 1\n",
    "        elif 'negative' in generated:\n",
    "            pred = 0\n",
    "        else:\n",
    "            pred = random.randint(0, 1)\n",
    "            invalid_count += 1\n",
    "        predictions.append(pred)\n",
    "        labels.append(example['label'])\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'invalid_count': invalid_count,\n",
    "        'predictions': predictions\n",
    "    }\n",
    "\n",
    "prompting_results_135m = evaluate_model_prompting(\n",
    "    gen_model, gen_tokenizer, dataset['test'],\n",
    "    best_prompt_template, \"SmolLM-135M\"\n",
    ")\n",
    "\n",
    "print(f\"\\nTest Set Results:\")\n",
    "print(f\"Accuracy:  {prompting_results_135m['accuracy']:.4f}\")\n",
    "print(f\"Precision: {prompting_results_135m['precision']:.4f}\")\n",
    "print(f\"Recall:    {prompting_results_135m['recall']:.4f}\")\n",
    "print(f\"F1 Score:  {prompting_results_135m['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Fine-tuning a generative model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing datasets\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1adda29fd061458c882fe05bf5f12e27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8530 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fea0a747b1846738143f9de365a4f94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 8530\n",
      "Validation samples: 1066\n"
     ]
    }
   ],
   "source": [
    "if gen_tokenizer.pad_token is None:\n",
    "    gen_tokenizer.pad_token = gen_tokenizer.eos_token\n",
    "\n",
    "# Preparing training data\n",
    "def prepare_gen_data(examples):\n",
    "    texts = []\n",
    "    for text, label in zip(examples['text'], examples['label']):\n",
    "        label_text = \"positive\" if label == 1 else \"negative\"\n",
    "        formatted = f\"Classify this movie review sentiment as positive or negative.\\n\\nReview: {text}\\nSentiment: {label_text}{gen_tokenizer.eos_token}\"\n",
    "        texts.append(formatted)\n",
    "    tokenized = gen_tokenizer(texts, truncation=True, max_length=256, padding='max_length')\n",
    "    tokenized['labels'] = tokenized['input_ids'].copy()\n",
    "    return tokenized\n",
    "\n",
    "print(\"Tokenizing datasets\\n\")\n",
    "gen_train_dataset = dataset['train'].map(prepare_gen_data, batched=True, remove_columns=dataset['train'].column_names)\n",
    "gen_val_dataset = dataset['validation'].map(prepare_gen_data, batched=True, remove_columns=dataset['validation'].column_names)\n",
    "\n",
    "gen_train_dataset.set_format('torch')\n",
    "gen_val_dataset.set_format('torch')\n",
    "\n",
    "print(f\"Training samples: {len(gen_train_dataset)}\")\n",
    "print(f\"Validation samples: {len(gen_val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training generative model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3201' max='3201' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3201/3201 38:19, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.392612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.372200</td>\n",
       "      <td>0.388089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.352300</td>\n",
       "      <td>0.388228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Your code for hyperparameter optimization here\n",
    "\n",
    "#hyperparameters used\n",
    "lr = 2e-5\n",
    "bs = 8\n",
    "epochs = 3\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    num_train_epochs=epochs,\n",
    "    per_device_train_batch_size=bs,\n",
    "    per_device_eval_batch_size=bs,\n",
    "    learning_rate=lr,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Reloading the model again\n",
    "gen_model = AutoModelForCausalLM.from_pretrained(gen_model_name).to(device)\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=gen_model,\n",
    "    args=training_args,\n",
    "    train_dataset=gen_train_dataset,\n",
    "    eval_dataset=gen_val_dataset,\n",
    ")\n",
    "\n",
    "# Model training\n",
    "print(\"\\nTraining generative model...\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on 1066 test examples:\n",
      "\n",
      "\n",
      "Test Set Results:\n",
      "Accuracy:  0.8630\n",
      "Precision: 0.8772\n",
      "Recall:    0.8443\n",
      "F1 Score:  0.8604\n"
     ]
    }
   ],
   "source": [
    "def evaluate_finetuned_gen(model, tokenizer, dataset_split):\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    invalid_count = 0\n",
    "    model.eval()\n",
    "    print(f\"\\nEvaluating on {len(dataset_split)} test examples:\")\n",
    "    for i, example in enumerate(dataset_split):\n",
    "        if i % 100 == 0:\n",
    "            print(f\"  Progress: {i}/{len(dataset_split)}\", end='\\r')\n",
    "        prompt = f\"Classify this movie review sentiment as positive or negative.\\n\\nReview: {example['text']}\\nSentiment:\" # only different from the prompting code\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=5,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.pad_token_id\n",
    "            )\n",
    "        generated = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True).strip().lower()\n",
    "        if 'positive' in generated:\n",
    "            pred = 1\n",
    "        elif 'negative' in generated:\n",
    "            pred = 0\n",
    "        else:\n",
    "            pred = random.randint(0, 1)\n",
    "            invalid_count += 1\n",
    "        predictions.append(pred)\n",
    "        labels.append(example['label'])\n",
    "\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "# Evaluate on test set\n",
    "finetuned_gen_results_135m = evaluate_finetuned_gen(gen_model, gen_tokenizer, dataset['test'])\n",
    "\n",
    "print(f\"\\n\\nTest Set Results:\")\n",
    "print(f\"Accuracy:  {finetuned_gen_results_135m['accuracy']:.4f}\")\n",
    "print(f\"Precision: {finetuned_gen_results_135m['precision']:.4f}\")\n",
    "print(f\"Recall:    {finetuned_gen_results_135m['recall']:.4f}\")\n",
    "print(f\"F1 Score:  {finetuned_gen_results_135m['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Fine-tuning a bidirectional model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: google-bert/bert-base-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizing datasets for BERT:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ec33c39633547fc852405c471532e29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 8530\n",
      "Validation samples: 1066\n"
     ]
    }
   ],
   "source": [
    "# Your code to train the transformer-based model on the training set and evaluate the performance on the validation set here\n",
    "\n",
    "bert_model_name = \"google-bert/bert-base-cased\"\n",
    "print(f\"\\nModel: {bert_model_name}\")\n",
    "\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "bert_model = AutoModelForSequenceClassification.from_pretrained(bert_model_name,num_labels=2).to(device)\n",
    "\n",
    "# Preparing data\n",
    "def prepare_bert_data(examples):\n",
    "    return bert_tokenizer(examples['text'], truncation=True, max_length=128, padding='max_length')\n",
    "\n",
    "print(\"\\nTokenizing datasets for BERT:\")\n",
    "bert_train_dataset = dataset['train'].map(prepare_bert_data, batched=True)\n",
    "bert_val_dataset = dataset['validation'].map(prepare_bert_data, batched=True)\n",
    "bert_test_dataset = dataset['test'].map(prepare_bert_data, batched=True)\n",
    "\n",
    "bert_train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "bert_val_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "bert_test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "print(f\"Training samples: {len(bert_train_dataset)}\")\n",
    "print(f\"Validation samples: {len(bert_val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1602' max='1602' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1602/1602 10:09, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.345200</td>\n",
       "      <td>0.349173</td>\n",
       "      <td>0.858349</td>\n",
       "      <td>0.865778</td>\n",
       "      <td>0.822635</td>\n",
       "      <td>0.913696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.203800</td>\n",
       "      <td>0.412670</td>\n",
       "      <td>0.875235</td>\n",
       "      <td>0.874647</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.870544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.079400</td>\n",
       "      <td>0.590018</td>\n",
       "      <td>0.860225</td>\n",
       "      <td>0.861653</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.870544</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Your code for hyperparameter optimization here\n",
    "\n",
    "# Used hyperparameters\n",
    "lr = 2e-5\n",
    "bs = 16\n",
    "epochs = 3\n",
    "\n",
    "# Defining metrics (optional)\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "bert_training_args = TrainingArguments(\n",
    "    num_train_epochs=epochs,\n",
    "    per_device_train_batch_size=bs,\n",
    "    per_device_eval_batch_size=bs,\n",
    "    learning_rate=lr,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "bert_trainer = Trainer(\n",
    "    model=bert_model,\n",
    "    args=bert_training_args,\n",
    "    train_dataset=bert_train_dataset,\n",
    "    eval_dataset=bert_val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "bert_trainer.train()\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set Results\n",
      ":\n",
      "Accuracy:  0.8480\n",
      "Precision: 0.8673\n",
      "Recall:    0.8218\n",
      "F1 Score:  0.8439\n"
     ]
    }
   ],
   "source": [
    "# Your code to evaluate the final model on the test set here\n",
    "\n",
    "test_results = bert_trainer.predict(bert_test_dataset)\n",
    "\n",
    "finetuned = {\n",
    "    'accuracy': test_results.metrics['test_accuracy'],\n",
    "    'precision': test_results.metrics['test_precision'],\n",
    "    'recall': test_results.metrics['test_recall'],\n",
    "    'f1': test_results.metrics['test_f1'],\n",
    "    'predictions': test_results.predictions.argmax(-1).tolist()\n",
    "}\n",
    "\n",
    "print(f\"\\nTest Set Results\\n:\")\n",
    "print(f\"Accuracy:  {finetuned['accuracy']:.4f}\")\n",
    "print(f\"Precision: {finetuned['precision']:.4f}\")\n",
    "print(f\"Recall:    {finetuned['recall']:.4f}\")\n",
    "print(f\"F1 Score:  {finetuned['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Bonus Task (optional)\n",
    "\n",
    "Repeat sections 3 through 5 here for a second generative and a second bidirectional model. When summarizing your results below (Section 7), include also comparison of the two generative models and the two bidirectional models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 SmolLM-360M-Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.1 Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: HuggingFaceTB/SmolLM-360M-Instruct\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b6079f3c0f84205afe47e02c1a700c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1614ad6b596e4202b4139e66337c18b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3860e790595643c4a1c7c637612af522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "691b8e41bbf940309e6f91760bc24c64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecdd36688a7c4399b390755f5420e7a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/565 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5b3e53938a04c58ae860fb07818ac40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/724 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "470912c92dcc4e2f91e6f7c1f27abbd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/724M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cfebc1d005c4f9c998564f9c623e8d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/156 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating SmolLM-360M with prompting on 1066 examples.\n",
      "\n",
      "Test Set Results\n",
      ":\n",
      "Accuracy:  0.8255\n",
      "Precision: 0.8916\n",
      "Recall:    0.7411\n",
      "F1 Score:  0.8094\n"
     ]
    }
   ],
   "source": [
    "gen2_model_name = \"HuggingFaceTB/SmolLM-360M-Instruct\"\n",
    "print(f\"\\nModel: {gen2_model_name}\")\n",
    "\n",
    "gen2_tokenizer = AutoTokenizer.from_pretrained(gen2_model_name)\n",
    "gen2_model = AutoModelForCausalLM.from_pretrained(gen2_model_name).to(device)\n",
    "\n",
    "if gen2_tokenizer.pad_token is None:\n",
    "    gen2_tokenizer.pad_token = gen2_tokenizer.eos_token\n",
    "\n",
    "prompting_results_360m = evaluate_model_prompting(\n",
    "    gen2_model, gen2_tokenizer, dataset['test'],\n",
    "    best_prompt_template, \"SmolLM-360M\"\n",
    ")\n",
    "\n",
    "print(f\"\\nTest Set Results\\n:\")\n",
    "print(f\"Accuracy:  {prompting_results_360m['accuracy']:.4f}\")\n",
    "print(f\"Precision: {prompting_results_360m['precision']:.4f}\")\n",
    "print(f\"Recall:    {prompting_results_360m['recall']:.4f}\")\n",
    "print(f\"F1 Score:  {prompting_results_360m['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  6.1.2 Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  6.1.2.1 Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing datasets for SmolLM-360M:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3654c31776c4205996763749492ed38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8530 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "008fb30232a64110b9971a39c1622e39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dataset preparation\n",
    "\n",
    "def prepare_gen2_data(examples):\n",
    "    texts = []\n",
    "    for text, label in zip(examples['text'], examples['label']):\n",
    "        label_text = \"positive\" if label == 1 else \"negative\"\n",
    "        formatted = f\"Classify this movie review sentiment as positive or negative.\\n\\nReview: {text}\\nSentiment: {label_text}{gen2_tokenizer.eos_token}\"\n",
    "        texts.append(formatted)\n",
    "    tokenized = gen2_tokenizer(texts, truncation=True, max_length=128, padding='max_length')\n",
    "    tokenized['labels'] = tokenized['input_ids'].copy()\n",
    "    return tokenized\n",
    "\n",
    "print(\"Tokenizing datasets for SmolLM-360M:\")\n",
    "gen2_train_dataset = dataset['train'].map(prepare_gen2_data, batched=True, remove_columns=dataset['train'].column_names)\n",
    "gen2_val_dataset = dataset['validation'].map(prepare_gen2_data, batched=True, remove_columns=dataset['validation'].column_names)\n",
    "\n",
    "gen2_train_dataset.set_format('torch')\n",
    "gen2_val_dataset.set_format('torch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.1.2.2 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Model\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1068' max='1068' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1068/1068 33:01, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.754900</td>\n",
       "      <td>0.741505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.711300</td>\n",
       "      <td>0.736643</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1068, training_loss=0.7614618187093556, metrics={'train_runtime': 1982.7381, 'train_samples_per_second': 8.604, 'train_steps_per_second': 0.539, 'total_flos': 4122375561216000.0, 'train_loss': 0.7614618187093556, 'epoch': 2.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training arguments\n",
    "gen2_training_args = TrainingArguments(\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=bs,\n",
    "    per_device_eval_batch_size=bs,\n",
    "    learning_rate=lr,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "gen2_trainer = Trainer(\n",
    "    model=gen2_model,\n",
    "    args=gen2_training_args,\n",
    "    train_dataset=gen2_train_dataset,\n",
    "    eval_dataset=gen2_val_dataset,\n",
    ")\n",
    "\n",
    "print(\"\\nTraining Model\")\n",
    "gen2_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.1.2.3 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on 1066 test examples:\n",
      "\n",
      "Test Set Results:\n",
      "\n",
      "  Accuracy:  0.8865\n",
      "  Precision: 0.9023\n",
      "  Recall:    0.8668\n",
      "  F1 Score:  0.8842\n"
     ]
    }
   ],
   "source": [
    "finetuned_gen_results_360m = evaluate_finetuned_gen(gen2_model, gen2_tokenizer, dataset['test'])\n",
    "\n",
    "print(f\"\\nTest Set Results:\\n\")\n",
    "print(f\"  Accuracy:  {finetuned_gen_results_360m['accuracy']:.4f}\")\n",
    "print(f\"  Precision: {finetuned_gen_results_360m['precision']:.4f}\")\n",
    "print(f\"  Recall:    {finetuned_gen_results_360m['recall']:.4f}\")\n",
    "print(f\"  F1 Score:  {finetuned_gen_results_360m['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 DistilBERT base-cased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.1 Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: distilbert/distilbert-base-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizing datasets for BERT:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a04418c8137c45649b0d585c85854f3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 8530\n",
      "Validation samples: 1066\n"
     ]
    }
   ],
   "source": [
    "bert_model_name = \"distilbert/distilbert-base-cased\"\n",
    "print(f\"\\nModel: {bert_model_name}\")\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "bert_model = AutoModelForSequenceClassification.from_pretrained(bert_model_name,num_labels=2).to(device)\n",
    "\n",
    "# Prepare BERT data\n",
    "def prepare_bert_data(examples):\n",
    "    return bert_tokenizer(examples['text'], truncation=True, max_length=128, padding='max_length')\n",
    "\n",
    "print(\"\\nTokenizing datasets for BERT:\")\n",
    "bert_train_dataset = dataset['train'].map(prepare_bert_data, batched=True)\n",
    "bert_val_dataset = dataset['validation'].map(prepare_bert_data, batched=True)\n",
    "bert_test_dataset = dataset['test'].map(prepare_bert_data, batched=True)\n",
    "\n",
    "bert_train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "bert_val_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "bert_test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "print(f\"Training samples: {len(bert_train_dataset)}\")\n",
    "print(f\"Validation samples: {len(bert_val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.2 Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.2.2.1 Configuration and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Configuration:\n",
      "Epochs: 3\n",
      "Batch size: 8\n",
      "Learning rate: 2e-05\n",
      "Mixed precision (FP16): True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3201' max='3201' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3201/3201 03:24, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.386600</td>\n",
       "      <td>0.395504</td>\n",
       "      <td>0.825516</td>\n",
       "      <td>0.831826</td>\n",
       "      <td>0.802792</td>\n",
       "      <td>0.863039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.253400</td>\n",
       "      <td>0.629152</td>\n",
       "      <td>0.834897</td>\n",
       "      <td>0.840290</td>\n",
       "      <td>0.813708</td>\n",
       "      <td>0.868668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.157500</td>\n",
       "      <td>0.755594</td>\n",
       "      <td>0.834897</td>\n",
       "      <td>0.838828</td>\n",
       "      <td>0.819320</td>\n",
       "      <td>0.859287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Completed.\n"
     ]
    }
   ],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average='binary'\n",
    "    )\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=2e-5,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=2,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    seed=42,\n",
    "    report_to=\"none\",\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "print(\"\\nTraining Configuration:\")\n",
    "print(f\"Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"Mixed precision (FP16): {training_args.fp16}\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=bert_model,\n",
    "    args=training_args,\n",
    "    train_dataset=bert_train_dataset,\n",
    "    eval_dataset=bert_val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "print('Training Completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.2.2.2 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set Results (BERT-base-cased):\n",
      "Accuracy:  0.8358\n",
      "Precision: 0.8303\n",
      "Recall:    0.8443\n",
      "F1 Score:  0.8372\n"
     ]
    }
   ],
   "source": [
    "test_results = trainer.predict(bert_test_dataset)\n",
    "\n",
    "finetuned_bert_results = {\n",
    "    'accuracy': test_results.metrics['test_accuracy'],\n",
    "    'precision': test_results.metrics['test_precision'],\n",
    "    'recall': test_results.metrics['test_recall'],\n",
    "    'f1': test_results.metrics['test_f1'],\n",
    "    'predictions': test_results.predictions.argmax(-1).tolist()\n",
    "}\n",
    "\n",
    "print(f\"\\nTest Set Results (BERT-base-cased):\")\n",
    "print(f\"Accuracy:  {finetuned_bert_results['accuracy']:.4f}\")\n",
    "print(f\"Precision: {finetuned_bert_results['precision']:.4f}\")\n",
    "print(f\"Recall:    {finetuned_bert_results['recall']:.4f}\")\n",
    "print(f\"F1 Score:  {finetuned_bert_results['f1']:.4f}\")\n",
    "\n",
    "del bert_model, trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 TurkuNLP/finnish-modernbert-large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3.1 Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: TurkuNLP/finnish-modernbert-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at TurkuNLP/finnish-modernbert-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizing datasets for BERT:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee49c237340944f4bd0d8234baedb77b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 8530\n",
      "Validation samples: 1066\n"
     ]
    }
   ],
   "source": [
    "bert_model_name = \"TurkuNLP/finnish-modernbert-large\"\n",
    "print(f\"\\nModel: {bert_model_name}\")\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "bert_model = AutoModelForSequenceClassification.from_pretrained(bert_model_name,num_labels=2).to(device)\n",
    "\n",
    "# Prepare BERT data\n",
    "def prepare_bert_data(examples):\n",
    "    return bert_tokenizer(examples['text'], truncation=True, max_length=128, padding='max_length')\n",
    "\n",
    "print(\"\\nTokenizing datasets for BERT:\")\n",
    "bert_train_dataset = dataset['train'].map(prepare_bert_data, batched=True)\n",
    "bert_val_dataset = dataset['validation'].map(prepare_bert_data, batched=True)\n",
    "bert_test_dataset = dataset['test'].map(prepare_bert_data, batched=True)\n",
    "\n",
    "bert_train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "bert_val_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "bert_test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "print(f\"Training samples: {len(bert_train_dataset)}\")\n",
    "print(f\"Validation samples: {len(bert_val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3.2 Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.3.2.1 Configuration and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Configuration:\n",
      "Epochs: 3\n",
      "Batch size: 8\n",
      "Learning rate: 2e-05\n",
      "Mixed precision (FP16): True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3201' max='3201' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3201/3201 22:17, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.302200</td>\n",
       "      <td>0.371642</td>\n",
       "      <td>0.867730</td>\n",
       "      <td>0.873769</td>\n",
       "      <td>0.835616</td>\n",
       "      <td>0.915572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.171500</td>\n",
       "      <td>0.543947</td>\n",
       "      <td>0.887430</td>\n",
       "      <td>0.886578</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.879925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.029800</td>\n",
       "      <td>0.841778</td>\n",
       "      <td>0.883677</td>\n",
       "      <td>0.882798</td>\n",
       "      <td>0.889524</td>\n",
       "      <td>0.876173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Completed.\n"
     ]
    }
   ],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average='binary'\n",
    "    )\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=2e-5,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=2,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    seed=42,\n",
    "    report_to=\"none\",\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "print(\"\\nTraining Configuration:\")\n",
    "print(f\"Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"Mixed precision (FP16): {training_args.fp16}\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=bert_model,\n",
    "    args=training_args,\n",
    "    train_dataset=bert_train_dataset,\n",
    "    eval_dataset=bert_val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "print('Training Completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.3.2.2 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set Results (TurkuNLP/finnish-modernbert-large):\n",
      "Accuracy:  0.8762\n",
      "Precision: 0.8848\n",
      "Recall:    0.8649\n",
      "F1 Score:  0.8748\n"
     ]
    }
   ],
   "source": [
    "test_results = trainer.predict(bert_test_dataset)\n",
    "\n",
    "finetuned_bert_results = {\n",
    "    'accuracy': test_results.metrics['test_accuracy'],\n",
    "    'precision': test_results.metrics['test_precision'],\n",
    "    'recall': test_results.metrics['test_recall'],\n",
    "    'f1': test_results.metrics['test_f1'],\n",
    "    'predictions': test_results.predictions.argmax(-1).tolist()\n",
    "}\n",
    "\n",
    "print(f\"\\nTest Set Results (TurkuNLP/finnish-modernbert-large):\")\n",
    "print(f\"Accuracy:  {finetuned_bert_results['accuracy']:.4f}\")\n",
    "print(f\"Precision: {finetuned_bert_results['precision']:.4f}\")\n",
    "print(f\"Recall:    {finetuned_bert_results['recall']:.4f}\")\n",
    "print(f\"F1 Score:  {finetuned_bert_results['f1']:.4f}\")\n",
    "\n",
    "del bert_model, trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Results and summary\n",
    "\n",
    "### 7.1 Corpus insights\n",
    "\n",
    "I learned how to prepare the datasets (already given) for any LM model and how to hyper-paramter the model because this is the main and important step before training our model. I heard before that training might goes for hours but I saw that thing in this project (that was highly frustrated every time I train the model). The corpus has 80% training, 10% testing and the reamining 10% for validation data. The testing data was well balanced before implementing any kind of the operation on it (means model prompting and fine-tunning).\n",
    "\n",
    "### 7.2 Results\n",
    "\n",
    "Recommended Models:\n",
    "\n",
    "*   While performing prompting on Smol-135M, the accuracy is 67.82%  and after fine-tunning the accuracy has increased to 86.30% (which was quite a bit interesting to me).\n",
    "*   BERT-cased got 84.80% accuracy.\n",
    "\n",
    "Now the Extra Models:\n",
    "\n",
    "\n",
    "*   After fine-tunning SmolLM-360M, got accuracy 88.65%\n",
    "*   Distil-BERT-cased got the accuracy of 83.58% after fine tunning the model.\n",
    "*   A finnish model was also tested: Finnish-mordernbert-large which gives the accuracy after fine-tunning the hyper-paramters is 87.62%. Finnish-mordernbert-small was also tested but the accuracy was around 80%.\n",
    "\n",
    "\n",
    "\n",
    "### 7.3 Relation to random baseline / expected performance / state of the art\n",
    "\n",
    "If I compare my results with the random baseline then my models perform a way better/ahead of the line. But when I compare my all the used models (in this project) to the expected performance. The results are following:\n",
    "\n",
    "*   SOTA has accuracy around 92%. While the highest accuracy I got after hyper-parametering my models is the 88.65% for SmolLM-360M-Instruct model. Which is less than the SOTA but I must say still not that bad.\n",
    "\n",
    " **I used FinBert just to check how the model performs and it went pretty well beyond by expectations.**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 Error analysis (group projects only)\n",
    "\n",
    "(Present the error analysis results here)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "D5d-9uxrcDY-"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
